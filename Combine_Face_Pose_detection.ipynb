{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import face_recognition\n",
    "import pyttsx3\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "import imutils\n",
    "import mediapipe as mp \n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score  \n",
    "import pickle\n",
    "import time\n",
    "import dlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Engine Functionn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine=pyttsx3.init()\n",
    "\n",
    "#Text to speech \n",
    "def speak(audio):\n",
    "    start_time = time.time()\n",
    "    engine.say(audio)\n",
    "    engine.runAndWait()\n",
    "    latency = time.time() - start_time\n",
    "   \n",
    "    print(\"Latency: \" + str(latency))\n",
    "\n",
    "    engine.stop(\n",
    "    \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with CustomObjectScope({'acc': ['acc']}):\n",
    "    model1 = load_model(\"C:/Users/MANSI YAGNIK/facerecog/face/mask.h5\")\n",
    "labels_dict = {0: 'without mask', 1: 'mask'}\n",
    "focal_length = 800\n",
    "object_width = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor_path = ('C:/Users/MANSI YAGNIK/facerecog/face/shape_predictor_68_face_landmarks.dat')\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "face_cascade = cv2.CascadeClassifier(\"face\\haarcascade_frontalface_default.xml\")\n",
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/MANSI YAGNIK/facerecog/face/faces'\n",
    "\n",
    "def findEncodings(path,cl):\n",
    "    image = os.path.join(path,cl)\n",
    "    reference_image = cv2.imread(image)\n",
    "    reference_gray = cv2.cvtColor(reference_image, cv2.COLOR_BGR2GRAY)\n",
    "    reference_landmarks1 = predictor(reference_gray, dlib.rectangle(0, 0, reference_gray.shape[1], reference_gray.shape[0]))\n",
    "    \n",
    "    \n",
    "    left_eye_template = [(reference_landmarks1.part(i).x, reference_landmarks1.part(i).y) for i in range(36, 42)]\n",
    "    left_eye_template_array = np.array(left_eye_template)\n",
    "    right_eye_template = [(reference_landmarks1.part(i).x, reference_landmarks1.part(i).y) for i in range(42, 48)]\n",
    "    right_eye_template_array=np.array(right_eye_template)\n",
    "    \n",
    "    left_eyebrow_template=[(reference_landmarks1.part(i).x, reference_landmarks1.part(i).y) for i in range(17, 22)]\n",
    "    left_eyebrow_template_array=np.array(left_eyebrow_template)\n",
    "    \n",
    "    right_eyebrow_template=[(reference_landmarks1.part(i).x, reference_landmarks1.part(i).y) for i in range(22, 27)]\n",
    "    right_eyebrow_template_array=np.array(right_eyebrow_template)\n",
    "    \n",
    "    \n",
    "    return left_eye_template_array, right_eye_template_array,left_eyebrow_template_array,right_eyebrow_template_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def faceAccuray(face_distance, face_threshold=0.6):\n",
    "    range = (1.0 - face_threshold)\n",
    "    linear_val = (1.0 - face_distance) / (range * 2.0)\n",
    "\n",
    "    if face_distance > face_threshold:\n",
    "        \n",
    "\n",
    "        return str(round(linear_val * 100, 2)) + '%'\n",
    "    else:\n",
    "        value = (linear_val + ((1.0 - linear_val) *\n",
    "                 math.pow((linear_val - 0.5) * 2, 0.2))) * 100\n",
    "        return str(round(value, 2)) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pose detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils  # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic  # Mediapipe Solutions\n",
    "\n",
    "df = pd.read_csv('C:/Users/MANSI YAGNIK/facerecog/face/coords1.csv')\n",
    "\n",
    "df[df['class'] == 'Stand with hands up']\n",
    "\n",
    "X = df.drop('class', axis=1)  # features\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "pipelines = {\n",
    "    # 'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc': make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}\n",
    "\n",
    "list(pipelines.values())[0]\n",
    "\n",
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train,y_train)\n",
    "    fit_models[algo] = model\n",
    "\n",
    "for algo, model in fit_models.items():\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))\n",
    "\n",
    "fit_models['rf'].predict(X_test)\n",
    "\n",
    "with open('body_language.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)\n",
    "\n",
    "with open('body_language.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognition:\n",
    "    face_locations = []\n",
    "    face_encodings = []\n",
    "    face_names = []\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    process_current_frame = True\n",
    "\n",
    "    def __init__(self):\n",
    "        self.encode_faces()\n",
    "\n",
    "    def encode_faces(self):\n",
    "        for image in os.listdir('faces'):\n",
    "            face_image = face_recognition.load_image_file(f\"faces/{image}\")\n",
    "            face_encoding = face_recognition.face_encodings(face_image)[0]\n",
    "\n",
    "            self.known_face_encodings.append(face_encoding)\n",
    "            self.known_face_names.append(image)\n",
    "        print(self.known_face_names)\n",
    "\n",
    "    def run_recognition(self):\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        for i in range(10):  # Try a range of indices to find supported resolutions\n",
    "            width = video_capture.get(3)  # Width\n",
    "            height = video_capture.get(4)# Height\n",
    "            print(\"Resolution\")\n",
    "            print(width)\n",
    "            print(height)\n",
    "        body_function=False\n",
    "        mask_fuction=False\n",
    "        function_executed=False\n",
    "        distance_fuc=False\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "            while True:\n",
    "                ret, frame = video_capture.read()\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = detector(gray)\n",
    "                small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "                rgb_small_frame = small_frame[:, :, ::-1]\n",
    "                gray_frame = cv2.cvtColor(rgb_small_frame, cv2.COLOR_BGR2GRAY)\n",
    "                normalized_frame = cv2.equalizeHist(gray_frame)\n",
    "                rgb_frame = cv2.cvtColor(normalized_frame, cv2.COLOR_GRAY2RGB)\n",
    "                results = holistic.process(rgb_frame)\n",
    "                self.face_locations = face_recognition.face_locations(\n",
    "                rgb_frame,number_of_times_to_upsample=3)\n",
    "                self.face_encodings = face_recognition.face_encodings(\n",
    "                rgb_frame, self.face_locations)\n",
    "                \n",
    "                print(rgb_frame.shape)\n",
    "                rgb_frame1 = cv2.resize(rgb_frame, (256, 256))\n",
    "                rgb_frame2 = np.expand_dims(rgb_frame1, axis=0)\n",
    "                result1 = model1.predict(rgb_frame2)\n",
    "                label = np.argmax(result1, axis=1)[0]\n",
    "                for face in faces:\n",
    "                   landmarks1 = predictor(gray, face)\n",
    "\n",
    "\n",
    "                   left_eye_landmarks1 = [(landmarks1.part(i).x, landmarks1.part(i).y) for i in range(36, 42)]\n",
    "                # print(left_eye_landmarks)\n",
    "                   left_eye_landmarks_array=np.array(left_eye_landmarks1)\n",
    "                \n",
    "                   right_eye_landmarks1 = [(landmarks1.part(i).x, landmarks1.part(i).y) for i in range(42, 48)]\n",
    "                # print(right_eye_landmarks)\n",
    "                   right_eye_landmarks_array=np.array(right_eye_landmarks1)\n",
    "                \n",
    "                   left_eyebrow_landmarks1=[(landmarks1.part(i).x, landmarks1.part(i).y) for i in range(17, 22)]\n",
    "                   left_eyebrow_landmarks_array=np.array(left_eyebrow_landmarks1)\n",
    "                \n",
    "                   rigth_eyebrow_landmarks1=[(landmarks1.part(i).x, landmarks1.part(i).y) for i in range(22, 27)]\n",
    "                   rigth_eyebrow_landmarks_array=np.array(rigth_eyebrow_landmarks1)\n",
    "                \n",
    "                   for cl in os.listdir(path):\n",
    "                        left_eye_template_array, right_eye_template_array,left_eyebrow_template_array,right_eyebrow_template_array = findEncodings(path, cl)\n",
    "\n",
    "                        euclidean_distanceLeft = np.linalg.norm(left_eye_landmarks_array - left_eye_template_array)/1000\n",
    "                        euclidean_distanceright = np.linalg.norm(right_eye_landmarks_array - right_eye_template_array)/1000\n",
    "                        euclidean_distanceLeft_eyebrow = np.linalg.norm(left_eyebrow_landmarks_array - left_eyebrow_template_array)/1000\n",
    "                        euclidean_distanceright_eyebrow = np.linalg.norm(rigth_eyebrow_landmarks_array - right_eyebrow_template_array)/1000\n",
    "                        print(euclidean_distanceright_eyebrow)\n",
    "                    \n",
    "                        threshold = 0.6\n",
    "                \n",
    "                        if euclidean_distanceLeft > threshold and euclidean_distanceLeft_eyebrow>threshold and euclidean_distanceright > threshold and euclidean_distanceright_eyebrow>threshold:\n",
    "                           name = os.path.splitext(cl)[0]\n",
    "                           print('name:', name)\n",
    "                           \n",
    "                           cv2.putText(frame, name, (face.left(), face.bottom() ), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                        \n",
    "                        \n",
    "                           print(\"Eyes matched!\")\n",
    "                        else:\n",
    "                           print(\"Eyes not matched.\")\n",
    "                        for landmark in landmarks1.parts():\n",
    "                           cv2.circle(frame, (landmark.x, landmark.y), 2, (0, 0, 255), -1)\n",
    "\n",
    "                print(label)\n",
    "                if not mask_fuction:\n",
    "                   speak( labels_dict[label])\n",
    "                   mask_fuction = True\n",
    "            \n",
    "            \n",
    "            \n",
    "                self.face_names = [] \n",
    "            \n",
    "                for face_encoding in self.face_encodings:\n",
    "                    matches = face_recognition.compare_faces(\n",
    "                    self.known_face_encodings, face_encoding)\n",
    "\n",
    "                # Calculate the shortest distance to face\n",
    "                    face_distances = face_recognition.face_distance(\n",
    "                        self.known_face_encodings, face_encoding)\n",
    "                    best_match_index = np.argmin(face_distances)\n",
    "                    if matches[best_match_index]:\n",
    "                        name = self.known_face_names[best_match_index].split('.')[0]\n",
    "                        confidence = faceAccuray(face_distances[best_match_index])\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    else:\n",
    "                    # matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
    "                        name = \"Unknown\"\n",
    "                        confidence = '???'\n",
    "                    self.face_names.append(f'{name} ({confidence})')\n",
    "                \n",
    "                    if not function_executed:\n",
    "                        speak(name)\n",
    "                        function_executed = True\n",
    "                    self.process_current_frame = not self.process_current_frame\n",
    "            # Display the results\n",
    "                for (top, right, bottom, left), name in zip(self.face_locations, self.face_names):\n",
    "                # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "                    top *= 4\n",
    "                    right *= 4\n",
    "                    bottom *= 4\n",
    "                    left *= 4\n",
    "                    rgb_frame.flags.writeable = True   \n",
    "                    rgb_frame1 = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "        # 2. Right hand\n",
    "                    mp_drawing.draw_landmarks(rgb_frame1, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        \n",
    "                    mp_drawing.draw_landmarks(rgb_frame1, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "                    mp_drawing.draw_landmarks(rgb_frame1, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "            # Grab ear coords\n",
    "                    pose = results.pose_landmarks.landmark\n",
    "                    print(pose)\n",
    "                    pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "            # Concate rows\n",
    "                    row = pose_row\n",
    "            # Make Detections\n",
    "                    X = pd.DataFrame([row])\n",
    "                    body_language_class = model.predict(X)[0]\n",
    "                    body_language_prob = model.predict_proba(X)[0]\n",
    "                    print(body_language_class, body_language_prob)\n",
    "                    coords = tuple(np.multiply(\n",
    "                            np.array(\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                        , [640,480]).astype(int))\n",
    "                    \n",
    "                    \n",
    "                    cv2.rectangle(frame, \n",
    "                          (coords[0], coords[1]+5), \n",
    "                          (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                          (245, 117, 16), -1)\n",
    "                    cv2.putText(frame, body_language_class, coords, \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Get status box\n",
    "                    cv2.rectangle(frame, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            # Display Class\n",
    "                    cv2.putText(frame, 'CLASS'\n",
    "                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(frame, body_language_class.split(' ')[0]\n",
    "                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display Probability\n",
    "                    cv2.putText(frame, 'PROB'\n",
    "                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(frame, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                \n",
    "                    face_width_pixels =left\n",
    "\n",
    "        # Calculate the distance to the face based on the size of the face in the image\n",
    "                    distance = (object_width * focal_length) / face_width_pixels \n",
    "                    distance=distance/305\n",
    "                    if not distance_fuc:\n",
    "                       speak(distance )\n",
    "                       speak(\"feet\" )\n",
    "\n",
    "                       distance_fuc = True\n",
    "                # Create the frame with the name\n",
    "                    cv2.rectangle(frame, (left, top),\n",
    "                              (right, bottom), (0, 0, 255), 2)\n",
    "                    cv2.rectangle(frame, (left, bottom - 35),\n",
    "                              (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "                    cv2.putText(frame, name, (left + 6, bottom - 6),\n",
    "                            cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "                    \n",
    "                    cv2.putText(frame, f\"Face Distance: {distance:.2f} feet\", (left, top +5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "                cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                   break\n",
    "\n",
    "        # Release handle to the webcam\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    fr = FaceRecognition()\n",
    "    fr.run_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
